---

title: "GAN(generative adversarial networks) (1)"
date: 2020-09-10 11:14 -0
last_modified_at: 2020-11-04 13:16 -0
categories: ML
comments: true

---

이 보관용 글은 GAN이 처음 소개된 Ian Goodfellow의 논문과 본인의 머릿속 내용을 정리한 것이다.


GAN은 generative model(생성모델)이다.

생성모델은 모델이 생성하는 샘플의 분포를 실제 데이터의 분포에 최대한 근사하도록 만드는 것이 목적이다.

쉽게 말하면 걍 진짜 같은 짜가를 만드는 게 목적이다. 에미야 시로?

GAN은 adversarial process(적대적 프로세스)를 통해 자신을 최적화시키는 생성모델이다.

GAN을 개발한 이안 굿펠로우의 비유에 따라 적대적 프로세스를 설명하자면, 적대적 프로세스는 지폐위조범과 경찰의 숨막히는 자강두천과 같다.

미숙한 지폐위조범이 위조지폐를 등신 같이 만들다 경찰한테 걸리면 깜방에 들어가게 되겠지.

하지만 깜빵 들어갔다고 해서 정신 차리고 바른 삶을 살아갈 우리의 지폐위조범이 아니다.

감옥에서 나온 지폐위조범은 이전의 경험을 바탕으로 더 그럴싸한 위조지폐를 만들기로 한다.

그러다 다시 경찰이 위조지폐가 시장에 돌아다닌다는 것을 알아채고 지폐위조범을 잡아가 깜방에 쳐넣는다.

그렇게 또 옥살이하다가 나온 지폐위조범은 다시 심기일전하여 더 나은 위조지폐를 만든다.

경찰은 위조지폐범이 만든 새 위조지폐를 보고 진짜인줄 알고 속을뻔하나, 겨우 알아채고 또다시 위조지폐범을 잡아낸다. 그리고 그가 만든 위조지폐에 속지 않기 위해 그가 만든 위조지폐의 특징을 파악하여 진짜 지폐와 구별할 수 있도록 학습한다.

그러나 경찰이 학습하는 동안, 위조지폐 제조자는 이미 더 나은 위조지폐를 만들어 경찰을 속이려고 하고 있었는데...!

하지만 경찰은 속지 않고, 위조지폐의 특징을 학습하고 구별해내며, 경찰에게 또 줘팸당한 위조지폐범은 다시 더 진짜 같은 위조지폐를 만들기 위해 다시 연구를 시작한다.

뭐 대충 이런 순환 구조가 계속 반복되는 것이다.

GAN에서는 경찰 역할을 하는 모듈을 분별자(discriminator), 위조지페 제조자 역할을 하는 모듈을 생성자(generator)라고 한다.

경찰에게 쳐맞고 와신상담하며 최고의 위조지폐 장인을 꿈꾸는 위조지폐 꿈나무(?)처럼, Generator는 Discriminator의 면박을 받으면서 점점 진짜에 가까운 데이터나 샘플을 생성해간다.


[![GAN is awesome](http://img.youtube.com/vidCKbRCUyop8/0.jpg)](http://www.youtube.com/watch?v=dCKbRCUyop8)

위의 영상은 현재의 GAN이 얼마나 발전했는지 보여준다. 영상에 나오는 GAN이 생성해낸 이미지는 실제에 가까우며, GAN의 latent space를 돌아댕기며 자신이 원하는 이미지를 생성할 수 있다.

하지만 현재 기술이 얼마나 발전했든지간에 그것이 내가 만든 GAN의 성능을 보장해주진 않는다 시발^^.


참고로 GAN은 Imitation Learning이라는 RL(Reinforcement Learning) 알고리즘의 특수한 형태 중의 하나인데, 솔직히 잘 모름 ㅋ.


앞에서 비유로 표현한 GAN의 학습 알고리즘을 구체적으로 설명하자면,

일단 생성자(generator) G가 z라는 확률변수(random variable)을 받아 가짜샘플을 만들어낸다.

앞의 비유에서는 지폐위조범이 위조지폐를 만드는 일과 동일하다.

그리고 분별자(discriminator) D는 입력값으로 현실에 존재하는 샘플과 생성자가 만들어낸 가짜 샘플을 받아, 그것이 얼마나 진짜 같은지를 0과 1 사이의 값으로 표현한다. 0에 가까울수록 가짜, 1에 가까울수록 진짜이다.

당연하게도 성능 좋은 분별자는 현실에 존재하는 샘플은 진짜라고 판단해야 하고(즉 출력값이 1에 가까워야 한다), 가짜 샘플은 가짜라고 판단해야 할 것이다(즉 출력값이 0에 가까워야 할 것이다).

응? 가짜만 잘 잡아내면 되지 않느냐고?

아니 진짜를 가짜라고 오판하지 않는 것도 중요하지. 위조지폐의 비유로 보면, 가짜를 잘 잡아내기만 하는 경찰은 구소련 비밀경찰들이나 중국 공안들과 다를 바가 없다. 반동분자 1명 잡으려고 일반인 100명도 함께 감옥에 집어처넣는거나 마찬가지다.

우리는 민주 시민들이 이끄는 자유민주주의 국가 대한민국에서 살기 때문에 그런 습근평평이 같은 짓을 해서는 안되는 것이다..!

워아이~~~ 베이지잉~~~~ 텐-안-먼~~.

그러니까... 그런 멍청한 소리는 그만하고!

우리는 현실의 샘플을 입력받은 D는 1에 가까운 값을, 생성자가 만들어낸 가짜 샘플을 입력받은 D는 0에 가끼운 값을 출력하도록 분별자를 최적화해야 한다.

그런 방향으로 D를 최적화시키는 방식에는 여러가지가 있지만, 최초의 GAN에서는 크로스엔트로피를 최소화하여 D를 최적화시키고자 했다.


즉, $$ \mathop{min}_D H(p_{data}, D) + H(p_{g}, 1 - D),$$ 
이때 $$p_{data}$$는 진짜 샘플들의 확률분포함수, $$ p_g $$는 생성자가 만들어내는 가짜 샘플들의 확률분포함수.


이 조건을 만족시키는 D가 최적화된 분별자라는 것이다.

분별자 D의 출력값이 '샘플이 진짜일 확률'의 추정값이라면, $$ 1 - D $$는 샘플이 진짜가 아닐, 즉 '가짜일 확률'의 추정값일 것이다.

크로스엔트로피를 다룬 게시글에서 말했듯이, 크로스엔트로피는 두 확률분포가 같을 때 최소화된다.

따라서 위 조건을 만족하는 분별자는 진짜 샘플을 입력받으면 1을 출력하고, 생성자가 만들어낸 가짜 샘플을 입력받으면 0을 출력하는, 이상적이고 모범적인 분별자일 것이다.

이상적인 분별자의 조건까지 이야기했으니, 그럼 이제 생성자에 대한 이야기를 해보자.

성능 좋은 생성자는 '진짜에 가까운' 가짜샘플을 만들어내는 생성자이다. 우수한 위조지폐범은 진짜 같은 위조지폐를 만드는 사람인 것처럼 말이다.

따라서 생성자가 얼마나 성능이 좋은지 알아보려면 생성자가 만들어낸 가짜샘플이 얼마나 현실의 샘플과 닮았는지 정량할 필요가 있다.

그러면 어떻게 해야 가짜샘플이 얼마나 진짜 같은지 정량할 수 있을까? 사람이 대충 눈으로 보고 점수를 매기면 될까?

그런.. 무식한 소리는 하면 안 되고!

사람이 일일히 점수를 매기는 일은 매우 귀찮은 일이고, 또한 객관성도 떨어진다. 그리고 무엇보다도 귀찮다.

그럼 대체 뭘 어쩌라는 것이냐.

아아, 답은 '깜방 갈 각오를 하고 위조지폐범 잡는 경찰한테 가서 이 위조지폐 얼마나 진짜 같냐고 물어본다.'이다.

여기서 말하는 경찰은 분별자 Discriminator이고, 위조지폐는 생성자 Generator가 만들어내는 진짜 샘플이다.

즉, 분별자에 가짜샘플을 입력하여 출력값이 1에 얼마나 가까운지를 보는 것이다.

최초의 GAN에서는 이를 정량하기 위해 또 크로스엔트로피를 사용했다.

즉, 최적화된 생성자는,

$$ \mathop{\max}_G H(p_g, 1 - D) $$ 를 만족하는 생성자이다.

다시 말해, '샘플이 가짜일 확률'에 대한 추정값인 $$ 1 - D $$와의 크로스엔트로피를 최대화하여 '분별자가 추정한 가짜샘플의 확률분포($$1-D$$)가 가짜샘플들의 실제 확률분포($$p_g$$)에서 멀어지도록 하는' 생성자가 최적화된 생성자이다.

쉽게 말하자면 분별자가 가짜가 아닐(진짜일) 가능성이 높다고 판별하는 가짜샘플들을 많이 맹글어내야 좋은 생성자라는 것이다.

자, 그럼 이제 분별자와 생성자 둘다 동시에 고려해보자.

분별자는 실제 샘플들의 분포와 그에 대한 추정값 사이의 크로스엔트로피 + 거짓된 샘플들의 분포와 그에 대한 추정값 사이의 크로스엔트로피를 최소화해야 하고,

생성자는 거짓된 샘플들의 확률분포와 그에 대한 추정값 사이의 크로스 엔트로피를 최대화해야 한다.

한쪽에선 크로스엔트로피를 최소화해야 하고, 다른 쪽에선 크로스엔트로피를 최대화해야 한다.

크로스엔트로피가 최소화되면서 동시에 최대화될 수는 없는 노릇이니, 분별자와 생성자는 마치 줄다리기하는 것처럼 크로스 엔트로피 값을 자신을 최적화하는 방향으로 움직이기 위해 서로 싸워야 한다.

이것이 바로 분별자와 생성자 간의 minimax game, 최소최대화 문제라는 것이다!

이를 수학적으로 표현하면 다음과 같다.

$$ \mathop{min}_G \, \mathop{max}_D V(G,D) = \mathop{\mathbb{E}}_{x \sim p_{data}} [\log D(x)] + \mathop{\mathbb{E}}_{z \sim p_z} [\log (1-D(G(z)))]$$

이 최소최대화 문제를 해결하는 과정에서 생성자는 실제 샘플들의 확률 분포를 경사하강법(gradient descent method)으로 학습해나간다.

마치 범죄자를 잡으려는 경찰과 안 잡히려는 위조지폐범간의 숨막히는 자강두천 대결 속에서 두 사람이 서로 절차탁마해나간 끝에 위조지폐범이 전설의 위조지페범이 되는 것처럼...!

훈훈-.


<br />
<br />
<br />

---
## GAN 알고리즘에 대한 이론적 증명
---


지금까지가 GAN의 작동기작에 대한 개략적인 설명이었다.

일단 이렇게 개략적인 원리만 보면 직관적으로 쉽게 납득할 수 있는 기계학습 알고리즘이다.

하지만 세상엔 엄근진한 태도로 직관에 칼을 들이대는 사람들도 있는 것이다..!

그래서 새로운 알고리즘을 제시할 땐 그 알고리즘이 작동할 수 있는 이유를 수학적으로 증명해야만 한다.

안 그러면 밤길에 수학귀신한테 칼빵 맞음.

GAN 논문의 저자도 당연히 GAN 알고리즘에 대한 수학적 증명을 써놨는데, 좀 야매로 증명해놨다.

어떤 야매를 써서 증명했냐면, '분별자와 생성자의 구조는 모든 확률분포함수를 나타낼 수 있을 만큼 강력하다.'라고 가정하는 야매를 썼다.

이는 당연하게도 현실과는 동떨어진 이야기다.

예를 들어, 생성자를 정규분포함수로 설정했다면 아무리 파라미터를 조정한다 하더라도 생성자가 연속균등확률분포를 나타낼 수가 없다.

그러므로 Generative Model에서 생성자가 생성할 수 있는 샘플의 분포는 생성자의 구조에 영향을 받을 수밖에 없다.

하지만 Dense ReLU layer로 이루어진 Neural Nets이라면 '사실상' 모든 확률분포함수를 나타낼 수 있을만큼 강력할 수 있다는 내용이 'Geometric Understanding of Deep Learning'이라는 논문에 나와있는데, 사실 아직 안 읽어봄 ㅋ

이에 대해선 나중에 리뷰하도록 해야겠다.

차치하고, 보통 어떤 알고리즘이나 문제해결법이 제대로 작동한다는 것을 보이려면 두 가지를 보여야 한다.

1. '원피스'는 존재한다.

2. 제시된 방법으로 '원피스'를 찾을 수 있다.

여기서 원피스는 우리가 원하는 문제의 '답'이다.

그럼 그냥 '문제의 해답이 존재한다.' 이렇게 쓰면 되지 왜 '원피스'라고 쓸까.

그냥 예전에 어느 교수님이 한 말씀이 인상에 남아서 그래... 수학에서 '해의 존재성'을 증명하는 것은 만화 원피스에서 해적왕 골드로져가 '원피스는 존재한다!!!'라고 말한 것과 같다고.

그 교수님이 참 원피스를 많이 좋아하셨는데... 출석 부를 때 이름이 진배인 학생을 맨날 '징베'라고 부르곤 했지...

어.. 이런 쓸데없는 소리를 많이 하면 안되고!

쨌든 GAN 알고리즘에 있어서, '원피스'와 '원피스 찾는 법'은 구체적으로 다음과 같다.

1. GAN의 minimax game은 Generator가 생성하는 샘플들이 현실의 샘플들과 같을 때 끝난다. 즉, 해당 minimax game은 $$ p_g = p_{data} $$일 때 optimal하다.

2. GAN의 Adversarial Process와 Gradient Descent Method가 $$ V(G,D) $$를 최적화시킬 수 있다.


<br />


### 1. $$ p_g = p_{data} $$ 일때 optimal하다.

이걸 증명하려면 두 가지 과정을 거쳐야 한다.

① 주어진 G에 대하여 $$V(G,D)$$가 최댓값을 가지는 것을 보인다.

② 위에서 말한, 주어진 G에 대한 $$V(G,D)$$의 최댓값을 $$C(G)$$라고 할 때, $$C(G)$$는 $$p_g = p_{data}$$일때 최솟값을 가진다.

그러면 안쪽의 max 조건, 바깥쪽의 min 조건이 모두 풀리게 되어 minimax game의 해를 구할 수 있다.

먼저 ①을 증명하도록 하자.

<div class="proposition" caption=1>
  For G fixed, the optimal discriminator D is 
  
  $$ D^* _G (x) = \frac{p_{data} (x)}{p_{data}(x) + p_g (x)} $$
</div>

<br />

여기서 optimal discriminator $$D^* _G (x)$$는 G가 고정된 값일 때 $$V(G,D)$$를 최대화하는 함수 D이다.

즉 Proposition 1은 G가 고정되어있을 때 $$V(G,D)$$는 최댓값을 가지며 그때 D는 $$D^* _G (x)$$라고 말하고 있는 것이다.

이에 대한 증명은 이렇다.

<div class="proof">
<br />
모든 \((a,b) \in \mathbb{R}^2 \setminus {0,0}\) 에 대하여 
함수 \(y \rightarrow a \log y + b \log (1-y)\)는 구간 \([0,1]\) (이 구간은 확률분포함수의 치역이다)에서 
\(y = \frac{a}{a+b}\)일 때 최댓값을 가진다는 것을 미분을 통해 쉽게 보일 수 있다.

연속확률변수에서의 기댓값(expectation)의 정의에 따라,

\begin{align}
V(G,D) &= \mathop{\mathbb{E}}_{x \sim p_{data} (x)} [\log D(x)] + \mathop{\mathbb{E}}_{z \sim p_z} [\log (1 - D(G(z)))] \\
 &= \mathop{\int}_x p_{data} (x) \log D(x) dx + \mathop{\int}_z p_z (z) \log (1 - D(G(z))) dx \\
 &= \mathop{\int}_x p_{data} (x) \log D(x) dx + \mathop{\int}_x p_g (x) \log (1 - D(x)) dx \\
 &= \mathop{\int}_x p_{data} (x) \log D(x) + p_g (x) \log (1 -D(x)) dx
\end{align}, 
<br />
where \(p_g\) is the probability distribution of the random variable of \(G(z)\).
<br />
함수 \(y \rightarrow a \log y + b \log (1-y)\)가 최댓값을 가지는 지점에서, \( a = p_{data}, b = p_g \) 일 때,
<br />
\(\mathop{\int}_x p_{data} (x) \log D(x) + p_g (x) \log (1 - D(x)) dx\) 또한 최댓값을 가지는 것이 자명(trivial)하므로,
<br />
\(V(G,D)\)는 \( D(x) = \frac{a}{a+b} = \frac{p_{data} (x)}{p_{data} (x) + p_g (x)} \) 일 때 최대가 된다.
<br />
corss-entropy는 확률변수의 서포트 영역에서만 정의되므로, 
\(D(x)\)와 다른 확률변수 \(x \sim p_{data} \), \( G(z) \sim p_g \) 사이의 cross-entropy에 마이너스 기호를 붙인 것과 마찬가지인 \(V(G,D)\)의 식은 \(supp (p_{data}) \cup supp (p_g)\)의 밖에서 정의되지 않는다.
<br />
따라서 우리는 \( a = p_{data} \), \( b = p_g \) 가 구간 \((0,1)\)에 있을 때만 생각하면 되므로, 이로써 증명이 완료되었다. 

</div>

<br />

자, 이렇게 ①을 증명했으니 이제 ②를 증명할 차례...이지만, 혹자는 

함수 \\(y \rightarrow a \log y + b \log (1-y)\\)가 최댓값을 가지는 지점에서, $$ a = p_{data} $$, $$ b = p_g $$ 일 때, 

$$ \mathop{\int}_x p_{data} (x) \log D(x) + p_g (x) \log (1 - D(x)) dx $$ 또한 최댓값을 가지는 것이 왜 자명한 것인지 의문을 품을 수도 있다.

$$a$$, $$b$$는 상수고 $$p_{data} (x)$$, $$p_g (x)$$는 값이 변하는 함수인데 말이야. 그리고 심지어 뒤의 수식은 log가 들어있긴 하지만 적분식임.

아 글쎄 trivial하다니깐?

하지만 설명은 해놓고 감.

설명문은 밑에 고이 접어두엇슴다.

<details>
<summary>눌러서 접힘 해제</summary>
<div markdown="1">
<br />

직관적인 설명은 이렇다. 적분은 리만합(Riemann Sum)의 극한값이므로, 아주 작은 구간(\\( [x_k, x_{k+1}] \\))에서 \\( \log \\) 함수 앞에 붙은 확률분포함수는 상수 취급해도 무방하다.

이렇게 상수가 된 확률분포함수를 \\( a = p_{data} (x^*) \\), \\( b = p_g (x^{**}) \\)이라고 하자.

이때 부분구간 \\( [x_k, x_{k+1}] \\) 에서 \\( a \log D(x) + b \log (1-D(x)) \\) 는 \\( D(x) = \frac{a}{a+b} \\) 일 때 최댓값을 가진다.

적분은 이러한 부분구간들에서의 함수값들의 합이고, 최댓값들을 전부 합한 값은 다시 함수값들의 합의 최댓값이 되므로, $$ D_G ^* (x) = \mathop{argmax}_{D} V(G,D) $$는 $$ \frac{a}{a+b} = \frac{p_{data} (x)}{p_{data} (x) + p_g (x)} $$ 가 된다.

사실 수학적 증명이 직관적인 설명보다 더 간단하다.

제1 적분 평균값 정리(First Mean Value Theorem For Integrals)와 적분 비교 정리(Comparison Theorm For Integrals)에 의해 이것이 증명된다.

후에 Mean Value Theorem For Integrals와 Comparison Theorem For Integrals 내용이 뭔지 또 까먹을 것에 분명한 미래의 본인을 위해 기재하자면,

<div class="theorem" caption=". First Mean Value Theorm For Integrals. ">
<br />

Suppose that \(f\) and \(g\) are integrable on \( [a,b] \) with \( g(x) \geq 0 ,\forall x \in [a,b] \).

If \( m = inf f[a,b] \) and \( M = sup f[a,b] \), then \( \exists c \in [m,M] \) such that \( \int _a ^b f(x) g(x) dx = c \int _a ^b g(x) dx \).

In particular, if \(f\) is continuous on \([a,b]\), \( \exists x_0 \in [a,b] \) which \( \int _a ^b f(x) g(x) dx = f(x_0) \int _a ^b g(x) dx \). 

</div>

<br />

\\( V(G,D) \\)의 경우에 있어선 \\( g(x) \\)는 \\( \log \\) 함수인데, \\( [0,1] \\)에서 이 로그 함수들은 음수이거나 0이다.

First Mean Value Theorem For Integrals는 \\( g(x) \geq 0\\)인 경우만 논하고 있기에 의문을 가질 수도 있지만, 사실 \\( g(x) \leq 0 \\) 일때도 성립한다. $$g(x)$$에 마이너스를 붙인 $$-g(x)$$는 First Mean Value Theorem For Integrals의 조건을 만족시키기 때문이다.

Comparison Theorem For Integrals는 이렇다.

<div class="theorem" caption=". Comparison Theorem For Integrals. " >
<br />

If \(f\), \(g\) is integrable on \([a,b]\) and \( f(x) \leq g(x)  \forall x \in [a,b] \), 

then \( \int _a ^b f(x) dx \leq \int _a ^b g(x) \).
</div>
<br />
$$ \mathop{\int}_x p_{data} (x) \log D(x) + p_g (x) \log (1 - D(x)) dx $$ 에 Mean Value Theorem을 적용하면 두 개의 확률분포함수가 상수가 되어서 적분기호 밖으로 튀어나오고, 그걸 다시 적분 기호 안으로 집어넣어 $$ y \rightarrow a \log y + b \log (1-y) $$를 적분하는 형태를 취한 다음 Comparison Theorem For Integrals를 적용하면, Q.E.D. 증명완료.

증명이 끝났으니까 이제 위로 올라가서 다시 이 내용을 접고 다시 본문 내용을 읽으면 된다.

음? 내 증명은 x, y가 실수일 때만 적용되는 증명이라고? x,y가 2차원 이상의 공간에 있을 때의 증명을 내놓으라고?

싫어요. 안되요. 안할거예요.


<br>
<br/>
</div>

</details>

<br />
이제 ②를 증명하자.

Proposition 1에 따라 $$G$$가 고정된 상태에서 최대화(Maximize)된 $$V(G,D)$$인 $$C(G)$$는,

$$
\begin{align}
C(G) &= \mathop{\max}_D V(G,D) \\
&= \mathop{\mathbb{E}}_{x \sim p_{data}}[\log D_G ^* (x)] +  \mathop{\mathbb{E}}_{z \sim p_{z}}[\log (1 - D_G ^* (G(z)))] \\
&= \mathop{\mathbb{E}}_{x \sim p_{data}}[\log D_G ^* (x)] +  \mathop{\mathbb{E}}_{x \sim p_{g}}[\log (1 - D_G ^* (x)] \\
&= \mathop{\mathbb{E}}_{x \sim p_{data} (x)}[\log \frac{p_{data}}{p_{data} (x) + p_g (x)}] + \mathop{\mathbb{E}}_{x \sim p_g}[\log \frac{p_g(x)}{p_{data} (x) + p_g (x)}]
\end{align}
$$

이다.

이때 우리가 증명해야 할 것은, 오직 $$p_g = p_{data}$$일 때 $$C(G)$$가 최소값을 가진다는 것이다.

즉, 다음을 증명해야 한다.

<div class="theorem" caption="1. ">
The global minimum of the virtual training creiterion \(C(G)\) is achieved if and only if \( p_g = p_{data} \). At that point, \(C(G)\) achieves the value \( - \log 4 \).
</div>

<br />

본래 if and only if 조건을 증명하려면 if와 only if 조건을 각각 따로따로 증명해내는 것이 정석이지만, 얘는 그냥 한방에 증명할 수 있다. 왜냐하면 마찬가지로 if and only if 조건이 붙어있는 다른 정리를 써서 얘를 증명할 것이기 때문이다.

KLD를 다룬 게시글에서 KL Divergence $$KLD(p\|q)$$는 오직 $$p=q$$일 때만(if and only if) 최소값 0을 가진다는 것을 언급했었는데, 이는 KL Divergence를 기반으로 정의된 JS Divergence도 마찬가지이다.

그렇다. 우리는 JS Divergence $$JSD(p\|q)$$가 오직 $$p=q$$일 때 최소값 0을 가진다는 것을 이용해 본 정리를 증명할 것이다.

<div class="proof">
<br />
\(
\begin{align}
C(G) &= \mathop{\mathbb{E}}_{x \sim p_{data} (x)}[\log \frac{p_{data}}{p_{data} (x) + p_g (x)}] + \mathop{\mathbb{E}}_{x \sim p_g}[\log \frac{p_g(x)}{p_{data} (x) + p_g (x)}] \\
&= \mathop{\mathbb{E}}_{x \sim p_{data} (x)}[\log \frac{1}{2} \frac{p_{data}}{\frac{p_{data} (x) + p_g (x)}{2}}] + \mathop{\mathbb{E}}_{x \sim p_g}[\log \frac{1}{2} \frac{p_g(x)}{\frac{p_{data} (x) + p_g (x)}{2}}] \\
&= \mathop{\mathbb{E}}_{x \sim p_{data} (x)}[\log \frac{p_{data}}{\frac{p_{data} (x) + p_g (x)}{2}}] + \mathop{\mathbb{E}}_{x \sim p_g}[\log \frac{p_g(x)}{\frac{p_{data} (x) + p_g (x)}{2}}] - \log 4 \\
\end{align}
\)
<br />
JS Divergence JSD의 정의에 따라,
<br />
\(
C(G) = - \log 4 + 2 JSD(p_{data}\|p_g)
\)
<br />
해당 JSD는 오직 \(p_{data} = p_g\)일 때 최소값 0을 가지므로, Q.E.D. 증명완료.

</div>
<br />
<br />
### 2.GAN의 알고리즘이 $$V(G,D)$$를 최적화시킬 수 있다.

이쯤에서 GAN의 알고리즘을 다시 리뷰해보자.

GAN의 알고리즘을 Pseudocode로 나타내보면 다음과 같다.

<div class="algorithm" caption=". Minibatch stochastic gradient descent training of generative adversarial nets. The number of steps to apply to the discriminator, k, is a hyperparameter. We used k-1, thel east expensive option, in our experiments. θ's are sets of parameters coding discriminator d and generater g, respectively.">
<strong>for</strong> number of training iterations <strong>do</strong>
<br />
&nbsp;&nbsp;<strong>for</strong> <i>k</i> steps do
<br />
&nbsp;&nbsp;&nbsp;&nbsp;Sample minibatch of \(m\) noise samples \( \{ z^{(1)}, ... , z^{(m)} \} \) from noise prior \(p_g (z)\).
<br />
&nbsp;&nbsp;&nbsp;&nbsp;Sample minibatch of \(m\) m examples \( \{ x^{(1)}, ..., x^{(m)} \} \) from data generating distribution \(p_{data} (x)\).
<br />
&nbsp;&nbsp;&nbsp;&nbsp;Update the discriminator by ascending its stochastic gradient:
$$
\nabla _{\theta _d} \frac{1}{m} \mathop{\Sigma}_{i=1} ^{m} [\log D(x^{(i)}) + \log(1 - D(G(z^{(i)})))].
$$
&nbsp;&nbsp;<strong>end for</strong>
<br />
&nbsp;&nbsp;Sample minibatch of \(m\) noise samples \( \{ z^{(1)}, ... , z^{(m)} \} \) from noise prior \(p_g (z)\).
<br />
&nbsp;&nbsp;Update the generator by descending its stochastic gradient:
$$
\nabla {\theta _g} \frac{1}{m} \mathop{\Sigma}_{i=1} ^{m} \log ((1 - D(G(z^{(i)})))).
$$
<strong>end for</strong>
<br />
The gradient-based updates can use any standard gradient-based learning rule. We used momentum in our experiments.
</div>

음? 뭔 소린지 잘 모르겠다구?

위에 영어로 써져있는 Theorem들은 잘만 읽었으면서, 뭘.

하지만 역시 후에 GAN의 이론적 배경에 대해 다 까먹어버릴 미래의 나를 위해 디테일한 내용을 적어둔다.

이 의사코드(Pseudocode)는 크게 두 가지 프로세스로 나뉘는 훈련과정을 반복하는데, 첫번째 프로세스는 Disciriminator를 훈련시키고 두번째 프로세스는 Generator를 훈련시킨다.

Disciriminator를 훈련시키는 과정은 이렇다. 일단 Generator가 가짜샘플을 $$m$$개 생성하게 하고, 마찬가지로 실제 데이터셋에서 데이터를 $$m$$개 샘플링한다. 그리고 가짜샘플들과 진짜 샘플들을 목적함수(objective function) $$V(G,D)$$에 대입하여, 목적함수를 최대화하도록 경사상승법(Gradient Ascent Method)을 시행한다. 그러면 목적함수가 최대화되는 방향으로, Discriminator를 규정하는 Parameter Set인 $$\theta _{d}$$가 수정된다.

그리고 이 훈련과정을 $$k$$번 반복한다. $$m$$과 $$k$$는 Hyperparameter로서 우리가 임의로 정해줘야 한다.

Discriminator의 훈련과정을 $$k$$번 반복한 후에 Generator를 '단 한번' 훈련시키는데, Discriminator와 마찬가지로 $$m$$개의 가짜샘플을 Generator가 만들게 해야 한다. 그 다음, 그 가짜샘플들을 $$V(G,D)$$에 대입하여 목적함수를 최소화하도록 경사하강법(Gradient Descent Method)을 시행한다. 그러면 목적함수가 최소화되는 방향으로, Generator를 규정하는 Parameter Set인 $$\theta _{g}$$가 수정된다.

이렇게 한 번의 Training iteration이 끝났다.

그럼 어떻게 되는거죠?

또다른 Training iteration이 시작되는거지. $$p_g$$가 $$p_{real}$$에 수렴할 때까지 반복하라구!

여기서 잠깐, 우리가 살펴보아야 할 부분이 있다. 한번의 트레이닝루프 동안에 Discriminator는 $$k$$번 파라미터가 업데이트되고, Generator는 단 1번만 파라미터가 업데이트된다.

이는 Theorem 1과 관련이 있다. Generator가 훈련시키기 전에 우리는 Proposition 1에 따라, G가 고정되어있을 때 $$V(G,D)$$를 최대화하여 Discriminator D를 최대값인수(Argument of Maxima)인 $$D_G ^* (x)$$로 만들 필요가 있다. 왜냐하면 Theorem 1은 오직, 주어진 G에 대해 목적함수가 최대화되어서 $$C(G)$$가 되었을 때만, 유일한 전역최소인수 $$p_g = p_{data}$$를 가진다고 보장하기 때문이다.

그래서 Generator보다 Discrimiator를 더 많이 훈련시켜, 해당 Generator에 대해 Discrimiator가 최대인수가 되도록 해야만 하는 것이다.

하지만 이것은 이론적인 이유고, D를 G보다 많이 훈련시켜야 하는, 현실적인 이유는 '모드 콜랩스(Mode Collapse)'를 방지하기 위함이다. Mode Collapse를 예를 들어 설명하자면 다음과 같다. 우리는 0부터 9까지의 숫자의 필기체를 생성하는 Generator를 훈련시키려고 한다. 이때 Discriminator를 충분히 훈련시키지 않아, Discriminator가 1~9까지의 숫자의 가짜샘플들은 잘 구분하지만 0의 가짜샘플은 잘 구분하지 못한다고 가정해보자. 그러면 굳이 1~9까지의 가짜샘플들을 생성하는 것보다 0의 가짜샘플들만 주구장창 만드는 것이 Discriminator를 속이기 더 쉬울 것이고, 결과적으로 0~9까지의 다양한 샘플들을 생성해내야만 하는 Generator는 0만 주구장창 생성해내는 단순한 Generator가 되어버리고 말 것이다. 이렇듯 Generator가 다양한 샘플들을 생성하지 않고 특정 패턴의 샘플들만을 생성하게 되는 현상이 Mode Collapse이다.

근데 이 파트는 분명 GAN 알고리즘을 통해 $$V(G,D)$$의 최적화가 가능하다는 것을 증명하는 파트인데 어째 GAN 알고리즘 설명이 너무 긴 거 같다?

그러므로 지금부터 슬슬 최적화 쉽가능을 증명하도록 하겠습니다.

Ian Goodfellow의 GAN 논문에서 최적화 쉽가능을 증명하는 부분은 $$\frac{1}{3}$$ 페이지조차 되지 않지만, 뭔 대학수학 교재 솔루션마냥 증명을 실전압축해놔서 이해하기가 쉽지 않다. 그냥 직관적으로 '대충 그럴 것 같네.'라고 느끼다가도, 어느새 '아니 근데 다시 보니까 이해가 안 가네?'라며 휘둥그래진 눈으로 다시 논문에 대가리를 쳐박고 증명을 살펴보게 되는 것이 참 악랄하지 않을 수가 없다.

어.. 근데 Ian Goodfellow의 논문을 토대로 GAN을 설명해놓은 다른 영상들이나 포스트들을 보면 이 증명을 어물쩍 넘기는게, 아무래도 나만 이해 못하는 것 같아요...

하지만 세계는 넓고 빡대가리들은 많으니, 분명 나처럼 본 논문의 증명과정을 이해 못하는 사람이 있을 수 있다고 믿고, 설명을 시작해보도록 하겠슴다.

일단 Ian Goodfellow의 GAN 논문에 적혀있는 증명은 다음과 같다.

<div class='proposition' caption=' 2. Convergence of Algorithm'>
<br />
If G and D have enough capacity, and at each step of Algorithm 1,
the discriminator is allowed to reach its optimum given G, and \(p_g\) is updated so as to improve the criterion
$$
\mathop{\mathbb{E}}_{x \sim p_{data}} [\log D _G ^* (x)] + \mathop{\mathbb{E}}_{x \sim p_g} [\log (1 - D _G ^* (x))]
$$
then \(p_g\) converges to \(p_{data}\)
</div>

<div class='proof'>
Consider \(V(G,D) = U(p_g, D)\) as a function of \(p_g\) as done in the above criterion. Note that \(U(p_g, D)\) is convex in \(p_g\). The subderivatives of a supremum of convex functions include the derivative of the function at the point where the maximum is attained. In other words, if \( f(x) = \mathop{\sup}_{\alpha \in \mathcal{A}} f_{\alpha} (x) \) and \(f_{\alpha} (x)\) is convex in \(x\) for every \(\alpha\), then \( \partial f_{\beta} \in \partial f\) if \(\beta = \mathop{argsup}_{\alpha \in \mathcal{A}} f_{\alpha} (x)\).
<br />
This is equivalent to computing a gradient descent update for \(p_g\) at the optimal \(D\) given the corresponding \(G\). \(\mathop{\sup}_D U{p_g, D}\) is convex in \(p_g\) with a unique global optima as proven in Thm 1, therefore with sufficiently small updates of \(p_g\), \(p_g\) converges to \(p_x\), concluding the proof.
</div>
<br />

Proposition 2 자체의 내용부터 차근차근 살펴보도록 하자.

앞에서 Ian Goodfellow가 증명과정에서 야매를 썼다고 했는데, 그게 바로 Proposition 2의 조건 중 하나인 'If G and D have enough capacity'이다. Generator와 Discriminator가 표현할 수 있는 확률분포함수의 가짓수가 훈련과정에 영향을 미치지 않을 정도로 충분히 많다는 이야기니까.

하지만 Ian Goodfellow가 쓴 야매는 이것만 있는 것이 아니다. 두 번째 조건인 'If the discriminator is allowed to reach its optimum given G at each step of Algorithm 1' 또한 야매이다. 우리가 GAN 알고리즘의 수렴성(최적화 쉽가능함)을 증명하려면 $$D$$의 최적화가 가능하다는 것과 $$G$$의 최적화가 가능하다는 것을 증명해야 하는데, 두번째 조건이 바로 'D의 최적화가 가능하다'라는 의미이기 때문이다.

사실 두 번째 조건을 야매로 보기엔 조금 무리가 있는데, $$D$$와 $$G$$가 충분한 capacity를 가지며, 매 훈련과정마다 반복되는 $$D$$의 학습반복횟수가 충분히 많다면 $$D$$는 Gradient Ascent Method에 의해 최적화될 수 있음이 자명(trivial)하기 때문에 그렇다.

하지만 원래 자명한 것이 제일 어려운 법이에요 그죠잉?

자명한 것은 설명도 안 해주기 때문에 '자명한 것이 자명함'을 빡대가리가 이해하려면 머가리를 풀가동하는 수밖에 없다.

$$D$$가 최적화될 수 있는 까닭은, 목표함수(Objective Function) $$V(G,D)$$가 $$D$$에 대하여 오목함수(Concave Function)이기 때문이다.

$$V(G,D)$$에 $$-$$를 붙인 $$-V(G,D)$$를 구성하는 두 개의 로그 함수는 앞에 $$-$$ 기호가 붙으면서 D에 대하여 볼록함수(Convex Function)이 되는데, 기댓값(Expectation) 연산은 확률변수에 대하여 선형(Linear)인 연산이라 확률변수의 볼록함(Convexity)을 보존한다. 따라서 $$-V(G,D)$$는 D에 대하여 convex하므로 $$V(G,D)$$는 concave하다.

그리고 concave하면 경사상승법(Gradient Ascent Method)에 의해 목표함수가 최대화될 수 있음이 보장된다.

반대로 convex하면 경사하강법(Gradient Descent Method)에 의해 최소화될 수 있음이 보장되고.

왜 그런지는 집에 가서 엄마와 함께 손으로 함수 그래프 그려가며 직접 경사상승법과 경사하강법을 실천해보면 쉽게 알 수 있으니까 나한테 묻지마셈.

아니, 훗날 내가 미래에 왜 convexity가 gradient descent method의 가능성을 보장해주는지 잊어먹을 수도 있으니까 나중에 포스트를 작성해두긴 할건데 여기서 설명하진 않을거다.

쨌든 이렇게 $$D$$의 최적화 가능성은 쉽게 이해할 수 있으므로, 우리가 증명해야 되는 것은 $$G$$의 최적화 가능성 뿐이다.

그리고 그게 Proof의 내용이다.

목표함수 $$V(G,D) = U(p_g, D)$$는 $$G$$ 혹은 $$p_g$$에 대하여 최소화되어야 하는데, 최소화가 가능함이 보장되려면 이게 $$p_g$$에 대해 convex해야만 한다.

기댓값 연산은 확률분포함수에 대하여 Linear하고, Linear한 연산은 Convex한 연산이므로 $$U(p_g, D)$$는 $$p_g$$에 대하여 convex하다.

응? 기댓값 연산이 확률분포함수에 대하여 Linear하다는 것이 뭔 뜻이냐구?

Linear하다는 것은 덧셈연산과 실수배 연산에 대하여 연산이 보존된다는 것을 의미한다, 이 말이야~~~~

응? 확률분포함수를 더하거나 실수배하면 적분해서 1이 되지 않으므로 확률분포함수가 아니라서 기댓값 연산이 먹히지 않는다구?

그런... 유도리 없는 소리는 하면 안되고!

Probability Axiom의 두번째 조건(확률은 총합이 1)을 무시하고 기댓값 연산에 대해 살펴보자.

$$
\begin{align}
\mathop{\mathbb{E}}_{x \sim p_1 + p_2} [f(x)] = \\
\mathop{\int}_x (p_1 (x) + p_2 (x)) f(x) dx = \\
\mathop{\int}_x p_1(x) f(x) dx + \mathop{\int}_x p_2(x) f(x) dx = \\
 \mathop{\mathbb{E}}_{x \sim p_1} [f(x)] + \mathop{\mathbb{E}}_{x \sim p_2} [f(x)]
\end{align}
$$

위와 같이 기댓값 연산은 확률분포함수의 덧셈연산을 보존하고, 실수배 연산 또한 같은 방법으로 보존됨을 보일 수 있다.

그럼 이제 다음 문장으로 넘어가보자. "Convex 함수들의 상한(supremum)의 subderivative에는, 함수값이 최대가 되는 점에서의 derivative가 포함된다(The subderivatives of a supremum of convex functions include the derivative of the function at the point where the maximum is attained)"라고 적혀있고 그 바로 다음 문장은 이 말을 좀 더 수학적으로 가다듬어서 표현한 것이다.

사실 이 두 문장의 의미는 아주 간단한데, 바로 $$C(G) = \mathop{\max}_{p_g} U(p_g, D)$$가 $$p_g$$에 대하여 convex하다는 뜻이다. 이걸 괜히 어렵게 써놔서 나는 한동안 이게 뭔 뜻인지 고민하느라 졸라 머가리가 깨져버릴 뻔했다... ㅅㅂ.

빡대가리가 이 두 문장의 의미를 직관적으로 파악하기 위해선 그래프를 그려보는게 직빵이지만 여분이 부족하므로 그래프는 다른 포스트에서 그리도록 하고, 그냥 말로 설명하자면 subderivative가 오직 convex function에 대해서만 정의되기 때문에 그렇다.

어쨌든간에, Wikipedia가 말하길, Convex 함수들의 집합에서의 $$\sup$$, $$\max$$ 연산은 Convexity를 보존하므로, 이 두 문장은 참말이다.

이렇게 두번째 문장과 세번째 문장의 의미를 이해하고 이것이 참임을 알 수 있었지만, 그래도 아직 빡대가리의 머릿속엔 의문이 남을 수 있다.

우리가 어떤 목적함수가 Convex 혹은 Concave 임을 보이는 이유는, 목적함수가 Convex 혹은 Concave여야만 Gradient Method를 통해 최소화 혹은 최대화가 가능하기 때문이다.

Gradient Method는 함수의 미분값, 즉 도함수(derivative)의 값에 따라 파라미터를 조정하는 방법이다.

하지만 여기선 subderivative에 대해서 논하고 있다. Subderivative는 '해당 Convex function 위의 한 점을 지나면서 항상 그 Convex function보다 아래에 있는 직선'의 기울기를 의미하는데, 대충 'Convex하지만 전구간에서 미분가능(differentiable)하지 않은' 함수들을 위한 도함수(derivative)라고 생각하면 된다. 미분불가능한 점에서 Subderivative의 정의를 만족하는 직선은 여러 개 있을 수 있으므로, 당연히 Subderivative는 여러 개 있을 수가 있고 그 중엔 평범한 도함수도 포함된다.

그리고 Derivative를 Subderivative로 대체해서 Convex function에 Gradient Descent Method를 적용하면, 해당 Convex function을 최소화할 수 있음을 알 수 있다.

그래프 그려보면 바로 이해가능하지만, 여기서 안 그릴거야.

어쨌든간에, 우리가 최소화하려는 $$C(G)$$엔 최대화($$\max$$) 연산이 포함되고, 미분가능한 함수들의 집합에 최대화 연산을 적용하면 그 결과가 미분가능하지 않을 수도 있다.

하지만 미분가능성(differentiablity)를 보존하지 않는 최대화 연산에 대해서도 Gradient Descent Method는 적용 가능한데, 최대화 연산이 적용된 함수에 대해 Gradient Descent Method를 사용할 때, 그 최대화된 함수 자체에 적용하는 것이 아니라, $$argmax$$ 연산을 이용해서 Derivative나 Gradient를 구하기 때문이다.

예를 들어 설명하자면 이렇다. 우리가 함수 $$h(x) = \max \{ f(x),g(x) \}, \, where \, f(x) = \frac{x}{2}, \, g(x) = x$$를 x에 대하여 최소화시켜야 한다고 해보자. $$h(x)$$는 convex function이지만 $$x=0$$에서 미분가능하지 않다. 하지만 Gradident Descent Method는 특정 $$x$$값에 대해서 $$argmax \{ f(x), g(x) \}$$인 함수를 미분해서 Derivative를 구하기 때문에, $$h(x)$$의 미분가능성은 별로 중요하지 않다. 그냥 $$f(x)$$, $$g(x)$$가 미분가능하기만 하면 된다.

따라서 $$x<0$$인 지점에선 $$f(x)$$의 미분값인 $$\frac{1}{2}$$이 도함수의 함수값이 되겠고, $$x>0$$인 지점에선 $$g(x)$$의 미분값인 $$1$$이 도함수의 함수값이 될 것이다. $$x=0$$인 지점에선 뭐 적당히 $$f(x)$$나 $$g(x)$$ 중 임의로 선택해서 구하겠지. 그리고 $$h(x)$$가 Convex function이기 때문에, $$x=0$$인 지점에서 두 함수 중 아무거나 골라 미분해도 둘다 $$h(x)$$의 subderivative가 된다.

그리고 앞에서 확인했듯이, Derivative를 Subderivative로 대체해서 Gradient Descent Method를 적용해도 함수를 최소화할 수 있다. 따라서 우리는 $$h(x)$$를 최소화할 수 있다. GAN 알고리즘의 경우엔, $$C(G)$$를 $$p_g$$에 대하여 최소화할 수 있다.

이것이 "This is equivalent to computing a gradient descent update for $$p_g$$ at the optimal $$D$$ given the corresponding $$G$$."의 의미이며, 앞서 $$C(G)$$가 $$p_g = p_{data}$$일 때 전역최소값(global minimum)을 얻는다고 증명했으므로, Gradient Descent Method에 의해 $$p_g$$는 $$p_{data}$$로 수렴할 수 있다.

자, 이것으로 Proposition 2의 증명이 끝났다.

<br />

이것으로 GAN 알고리즘의 이론적인 증명은 종료되었고, 이 다음 내용은 뭐 GAN의 실험결과나 장단점이나 보완할 점이라든가 그런 실질적인 내용인데, 애초에 GAN 논문의 GAN 알고리즘 증명 파트에서 중간과정을 너무 많이 생략해놔서 그 중간과정을 기록하려고 한 것이 이 포스트를 작성한 이유이므로, 이론적 증명 뒷 내용은... 미래의 내가 하겠지 뭐.

그럼 머-바-.(머신러닝 바이라는 뜻 ㅎ.)

아, 그전에 한 가지 더.

이론적으로 GAN의 Objective Function은 $$\mathop{\mathbb{E}}_{x \sim p_{data}} [\log D(x)] + \mathop{\mathbb{E}}_{x \sim p_z} [\log (1 - D(x))]$$이지만, 소레와 우소다.

나니잇?!

소레데와 잇따이 나니가 GAN노 Objective Function 나노?!

아아, 소레와...

$$\mathop{\mathbb{E}}_{x \sim p_{data}} [\log D(x)] - \mathop{\mathbb{E}}_{x \sim p_z} [\log D(x)]$$ 나노다.

오마에.. 와따시타치오 다마사레따?!

젯따이 유루사나이...!

아아, 야떼미로.

이런... 헛소리는 이제 그만 적고!

실제로 GAN을 학습시킬 때의 목적함수는 이론적으로 도출된 목적함수와 다르다.

그 이유는 $$D(G(z))$$가 0에 가까운 값일 때 $$\log (1-D(G(z)))$$의 기울기가 너무 작기 때문이다.

무려 기울기 값이 1이나 된다. 기울기값 단 1나!

$$D(G(z))$$가 0에 가깝다는 건 Generator가 너무 멍청해서 Discriminator를 아예 속일 수 없다는 것을 의미한다.

이럴 땐 학습강도를 높게 잡아서(즉 목적함수의 기울기를 최대한 높게 잡아서) 빨리빨리 Generator를 팍팍 학습시켜야 하는데, 목적함수의 기울기가 1이면 $$G$$ 새끼가 느리게 학습해도 되는 줄 알고 축 늘어져있어요 하이고.

하지만 $$ - \log D(G(z)) $$는 $$D(G(z))$$의 값이 0에 가까울 때 기울기 값이 $$+ \infty$$로 발산하므로, Generator를 빨리빨리 학습시킬 수 있다.

그렇기에 GAN을 실제로 학습시킬 땐 목적함수의 $$\log (1-D(G(z)))$$를 $$ - \log D(G(z)) $$로 교체해서 사용한다.

여담으로, LSGAN이라든가 WGAN 같이 보다 진보된 GAN에서는 최초의 GAN의 목적함수와는 사뭇 다른 목적함수를 사용하는데, 이는 기회가 있으면 다른 포스트에서 다루도록 한다.

그럼 진짜 머-바-!


