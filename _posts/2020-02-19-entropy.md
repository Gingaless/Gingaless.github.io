---

title: "엔트로피(Entropy)"
date: 2020-08-18 10:56 -0
categories: [ML]
comments: true
use_math: true
published: true

---


정보이론에서 정보량 I는 다음과 같이 정의된다.


$I(X) := -\log_b P(X) \, (=\log _b \frac{1}{P(X)})$, 여기서 $I(X)$는 샘플 X가 담고 있는 정보량, b는 임의의 밑(보통 2), $P(X)$는 X가 일어날 확률이다.

즉 어떤 사건이나 샘플이 희귀할수록(확률이 낮을수록) 가지고 있는 정보량이 많다.

예를 들어, 어떤 살인사건에 대한 목격자 진술이 '범인은 남자이다.'인 때보다 '범인은 양손이 다 오른손이다.'일 때 우리가 범인에 대해 더 많은 정보를 얻을 수 있다. 남성은 전인류의 50퍼센트나 되지만, 양손이 다 오른손인 사람은 매우 드물테니까.

그런데 우리는 여기서 '확률이 낮아짐에 따라 값이 증가하는 함수들은 많고 많은데 왜 하필 로그함수냐?'라고 물을 수 있는데, 그 이유는 b개의 문자를 가지고 n개의 상태를 표현하기 위해선 문자열의 길이가 $\log _b n$여야 하기 때문이라고 생각하면 편하다.

컴퓨터가 사용하는 2진법을 예로 들어보자.

만약 어떤 시스템의 상태가 딱 2가지 있다고 한다면, 0과 1 단 두 개의 문자만으로도 시스템의 모든 상태를 표기할 수 있다.

시스템의 상태가 4가지 있다고 한다면, 0과 1과 01과 11 이 4가지 문자열로 시스템의 상태를 표기해야 할 것이다. 이때 문자열의 최대길이는 2이다.

시스템의 상태가 8가지 있다고 한다면, 최대 길이가 3인 문자열로 시스템의 상태를 표기해야 할 것이다.

즉 우리가 어떤 시스템에 대한 정보를 표현하기 위해서는 그 시스템이 가질 수 있는 상태의 개수에 로그를 씌운 값이 최대 길이인 문자열들이 필요하다는 것이다.

이때 시스템이 특정 상태를 띨 확률이 모두 동일하다고 가정한다면, $\frac{1}{n}$이 시스템이 특정 상태를 띨 확률이 되며, 이때 시스템의 상태의 총 개수인 $n$은 그 확률의 역수가 된다.

따라서 그 시스템의 특정 상태에 대한 정보를 표현하기 위해 필요한 문자열(혹은 자원)의 양은,

$\log _2 n =  \log _2 \frac{1}{\frac{1}{n}} = \log \frac{1}{p _X (x)}, \text{이때 x는 시스템의 특정 상태.}$

이 되며, 이것을 우리가 '정보량'이라고 말하는 것이다.


그럼 이제 본 주제인 엔트로피에 대해 이야기를 해보자.

흔히 '무질서도'라고 일컬어지는 엔트로피 H(정확히 말하면 Shannon Entropy)의 정의는,

$H := - \sum_{i} p _X (x_i) \log p _X (x_i)$로, 시스템 전체(혹은 sample space 전체)의 정보량의 기댓값과 같다.

우리는 여기서 이런 의문을 품어볼 수 있다.

'정보와 무질서도는 완전히 정반대의 개념 같은데, 왜 무질서도인 엔트로피는 정보량의 기댓값인 것이지?'

그 이유는 간단하다. 어떤 샘플이 희귀해서 가지고 있는 정보량이 많다는 의미는, 어떤 샘플은 매우 전형적이어서 가지고 있는 정보량이 적다는 것을 의미하기 때문이다.

이때 희귀한 샘플로 인해 시스템 전체의 정보량이 증가하는 효과보다, 전형적인 샘플로 인해 시스템 전체의 정보량이 감소하는 효과가 더 크다.

그래서 엔트로피는 '모든 사건이 동일한 확률을 가질 때', 즉 시스템이 가장 무질서할 때 최대가 되는 것이다.


예를 들어, 동전 던지기를 생각해보자.

동전 던지기의 결과는 무작위적이기 때문에 앞면이 나올 확률, 뒷면이 나올 확률 모두 $\frac{1}{2}$이다.

이 경우 엔트로피를 계산해보면,

$-(\frac{1}{2} \log _2 \frac{1}{2} + \frac{1}{2} \log _2 \frac{1}{2}) = 1$ 이 된다.

그런데 만약에, 동전을 던져서 앞면이 나올 확률이 $\frac{3}{4}$가 되도록 동전을 던질 수 있는 달인이 있다고 해보자.

이때 엔트로피를 계산해보면,

$\frac{3}{4} \log_2 {\frac{4}{3}} + \frac{1}{4} \log_2 4 \approx 0.81$ 으로, 동전던지기가 완전히 무작위적일 때보다 크기가 작다.

이처럼, 좀 더 전형적인 경우(앞면)와 좀 더 희귀한 경우(뒷면)가 나뉘는, 질서있는 시스템에서보다 사건 발생이 완전히 무작위적인, 무질서한 시스템에서의 엔트로피가 더 크다.

