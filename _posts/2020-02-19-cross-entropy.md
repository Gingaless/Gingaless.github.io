---

title: "크로스 엔트로피(Cross-entropy)와 쿨백-라이블러 발산(KL Divergence)"
date: 2020-02-19 14:38:00 +0000
categories: ML
comments: true
use_math: true
published: true

---

확률분포 $p$에 대한 확률분포 $q$의 크로스 엔트로피 $H(p,q)$의 정의는 다음과 같다.

$H(p,q) = \sum _{x \in \chi} p(x) \log \frac{1}{q(x)}$, 이때 $\chi$는 확률분포 $p$의 서포트(support).

서포트는 쉽게 말하자면 그냥 함수값이 0이 아닌 구간이라고 생각하면 편하다.

크로스 엔트로피의 실제적 의미는 "p라는 확률분포를 가지는 표본공간(sample space)의 확률분포를 q라고 가정했을 경우 우리가 낭비(?)해야 하는 정보량(을 표현하기 위한 자원의 양)의 기댓값" 쯤 된다.

예를 들어, 어떤 학급에 과체중인 학생이 $\frac{3}{8}$, 정상 체중인 학생이 $\frac{3}{8}$, 저체중인 학생이 $\frac{1}{4}$의 비율로 있다고 하자.

그리고 반장이 자신의 학급 친구들의 체중 상태를 조사해야만 한다고 해보자.

반장은 학생들의 목록의 성명란 옆에 빗금을 쳐서 친구들의 체중 상태를 기록하기로 생각했다.

만약 반장이 저체중인 학생과 정상 체중인 학생과 과체중인 학생이 동일한 비율로 존재한다고 가정할 경우, 반장은 그냥 자기 보기에 편하게

저체중은 빗금 하나, 정상체중은 빗금 두 개, 과체중은 빗금 세 개로 표시하기로 할 것이다.

하지만 만약 반장이 급우들의 체중에 관한 정확한 확률분포를 알고 있고 그에게 볼펜 잉크를 최대한 아끼고자 하는 소명이 있다면, 가장 학생수가 적은 저체중에 빗금 세 개를 치려고 할 것이다. 그래야 잉크를 아낄 수 있기 때문이다.

깔쌈하게 그 학급에 학생이 8명 있다고 가정할 경우, 전자의 가정에 따라 빗금을 쳤을 때 총 빗금의 갯수는

$1 \times 2 \, (저체중) + 3 \times 2 \, (정상) + 3 \times 3 \,(과체중) = 2 + 6 + 9 = 17$

가 된다.

후자의 가정을 따랐을 땐,

$1 \times 3 + 3 \times 2 + 3 \times 1 = 3 + 6 + 3 = 12$ 로,

확률분포를 정확히 추정하였을 때가 그렇지 않을 때보다 볼펜의 잉크(정보를 표현하는 수단이나 자원)를 덜 쓸 수 있었다.

이와 같이 크로스 엔트로피는 실제의 확률분포 p와 가정상의 확률분포 q가 같을 때 최저가 되며, 그렇기에 생성 모델(generative models)의 손실함수(loss function)으로 사용된다.


그런데 아까 크로스 엔트로피가 '확률분포를 잘못 가정했을 경우 낭비해야 하는 정보량'이라고 말했었는데,

엄밀하게 말하면 구라다.

크로스 엔트로피는 확률분포를 잘못 가정했을 때에 그 값이 커지지만 그냥 실제의 확률분포 p의 엔트로피가 높을 때도 값이 크기 때문이다.

그래서 우리가 정말로 '확률분포를 잘못 가정했을 때 추가로 낭비하는 정보량'의 크기를 나타내기 위해선, 크로스 엔트로피에서 본래의 엔트로피 값을 빼줘야 한다.

그리고 그렇게 해서 나온 값을 우리는 쿨백-라이블러 발산(Kullback-Leibler Divergence), 줄여서 KLD라고 부른다.

KLD의 정확한 정의는 이렇다. 확률분포 p에 의한 q의 KLD $D _{KL} (p \| q)$는,

$D _{KL} (p \| q) := \sum _{x \in \chi} p(x) \log \frac{p(x)}{q(x)}, \, \text{$\chi$는 p의 support.}$

그리고 깁스 부등식(Gibbs' Inequality)에 따라 KLD는 항상 0보다 크거나 같으며, KLD의 값이 0일 경우는 오직 두 확률분포 p와 q가 같을 때뿐이다.

