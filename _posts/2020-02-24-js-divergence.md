---

title: "JS 발산(Jensen-Shannon Divergence)"
categories: [ML]
date: 2020-02-25 14:40:00 +0000
use_math: true
published: true

---

이제 옌센-샤논 발산(Jensen-Shannon Divergence), 줄여서 JSD에 대해 알아보도록 하자.

이것만 다 쓰고나면 드디어 GAN(Generative Adversarial Networks)에 대한 글을 쓸 수 있다...!

응? JSD의 JS를 보니까 뭔가 묘한 생각이 난다고?

자바스크립트 꺼라.

눈동자 꺼라.

난 대체 뭔 개소릴 하고 있는거지.

거두절미하고, JSD는 KL 발산의 '거리(metric)' 버전이다.

KL 발산은 실제의 확률분포 p가 주어졌을 때 그것에 근사하는 확률분포 q로 확률분포를 가정할 경우 손실되는 정보량이다.

즉 KL 발산에선 이미 더 우수한 확률분포가 이미 정해져있다. 실제이거나 아니면 적어도 실제에 가까울 것으로 추정돠는 확률분포 p이다.

하지만 현실에선 p와 q 둘중에 어느 것이 더 실제에 가까운지 알 수 없는 경우가 더 많다.

이럴 때 대등한 두 확률분포 p와 q가 서로 얼마나 비슷하고 얼마나 다른지를 수치로 나타낸 것이 JSD이다.

두 확률분포 p와 q 사이의 JSD는 다음과 같이 정의된다.

$JSD(p\|\|q) := D _{KL} (p\|\|m) + D _{KL} (q \|\| m), \text{where $m = \frac{p+q}{2}$}$

이러면 JSD는 대칭함수가 되어 거리함수(metric function)의 조건을 만족한다.

그런데 혹자는 이렇게 생각할 수도 있다.

'그냥 $D _{KL} (p \|\| q) + D _{KL} (q \|\| p)$ 해도 대칭함수 되는거 아님?'

그런 생각을 옛날에 제프리(Jeffrey)라는 양반도 했었나보다.

제프리 발산(Jeffrey's Divergence)이 바로 그렇게 정의된다.

그런데 제프리 발산은 p외 q의 값이 작을 때에 수치계산적인(numerical) 문제가 발생할 수 있는데다가 상한값(upper bound)도 없어서 좀 쓰기에 불편하댄다.

반면 JSD는 제프리 발산에서 발생할 수 있는 수치계산적 문제가 일어나지 않는데다가 상한값($\log 2$. 미분 때리면 바로 구할 수 잇음.)도 있어서 쓰기에 편하다 카더라.

잉? 거리가 왜 상한값을 갖냐고? 상한값을 가지고 있는게 뭔 거리냐고?

니는 이산공간(discrete space)도 안 배웠냐, 이눔아!

이산공간의 거리함수는 같은 점이면 0,다른 점이면 1을 함수값으로 갖기에 함수값의 상한값이 1이다.

JSD도 그런 맥락이라고 받아들이면 될 것 같다.

참고로 JSD나 제프리 발산 말고도 f-divergence라든지 두 확률분포함수 간의 차이를 나타내는 거리함수엔 여러가지가 있다.

하지만 초창기 GAN의 수렴성을 증명하는데엔 KLD와 JSD 정도만 알아도 충분하다.

이제 GAN에 대해서 써야징 룰루랄라

이 글은 킹갓엠페러위키피디아와 Nielsen, F(2019)의 논문 일부를 참조하여 쓰여졌으며 대부분은 뇌피셜입니다.
